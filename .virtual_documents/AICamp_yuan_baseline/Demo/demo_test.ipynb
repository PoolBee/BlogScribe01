import whisper
import os  
#whisper_model = whisper.load_model("./voice_model/large-v2")
file_path = "./voice_model/large-v2.pt"
if os.path.exists(file_path):  
    print("文件存在，可以加载模型。")  
    whisper_model = whisper.load_model(file_path)  
else:  
    print("文件不存在，请检查路径和文件名。")  

# 或者，列出voice_model目录下的所有文件以查找模型文件  
voice_model_dir = "./voice_model"  
if os.path.isdir(voice_model_dir):  
    print("voice_model目录下的文件：")  
    for filename in os.listdir(voice_model_dir):  
        print(filename)  
else:  
    print("voice_model目录不存在。")


from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader


if device == "NVIDIA A10":
    torch.cuda.empty_cache()


import torch  
  
# 检查CUDA是否可用  
if torch.cuda.is_available():  
    print("CUDA is available. Using GPU.")  
    # 获取CUDA设备数量  
    num_gpus = torch.cuda.device_count()  
    print(f"Number of GPUs available: {num_gpus}")  
    # 你可以通过torch.cuda.get_device_name(0)等方式获取特定GPU的名称  
    print(f"GPU name: {torch.cuda.get_device_name(0)}")  
else:  
    print("CUDA is not available. Using CPU.")


#出现后RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
#请用清除确保在每次运行程序之前释放GPU资源。您可以在程序结束时使用以下代码释放显存：
import torch  
  
# 检查CUDA是否可用  
if torch.cuda.is_available():  
    print("CUDA is available. Using GPU.")  
    # 在这里进行一些GPU操作，比如创建一些张量  
    # ...  
      
    # 当你不再需要这些GPU资源，或者想要释放一些缓存内存时  
    # 调用 torch.cuda.empty_cache()  
    torch.cuda.empty_cache()  
    print("CUDA cache cleared.")  
else:  
    print("CUDA is not available. Using CPU.")


pip install soundfile



pip install --upgrade modelscope




pip install pyperclip
