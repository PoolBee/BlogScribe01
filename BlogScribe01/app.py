import torch
import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import LLMChain
from langchain.llms.base import LLM
from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from typing import Any, List, Optional
import whisper

import tempfile
from modelscope import snapshot_download

# å‘é‡æ¨¡å‹ä¸‹è½½
model_dir = snapshot_download('AI-ModelScope/bge-small-zh-v1.5', cache_dir='./')

# æºå¤§æ¨¡å‹ä¸‹è½½
model_dir = snapshot_download('IEITYuan/Yuan2-2B-Mars-hf', cache_dir='./')

# å®šä¹‰æ¨¡å‹è·¯å¾„
model_path = './IEITYuan/Yuan2-2B-Mars-hf'

# å®šä¹‰å‘é‡æ¨¡å‹è·¯å¾„
embedding_model_path = './AI-ModelScope/bge-small-zh-v1___5'

# å®šä¹‰æ¨¡å‹æ•°æ®ç±»å‹
torch_dtype = torch.bfloat16 # A10

class Yuan2_LLM(LLM):
    tokenizer: AutoTokenizer = None
    model: AutoModelForCausalLM = None

    def __init__(self, mode_path: str):
        super().__init__()
        print("Creat tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(mode_path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')
        self.tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>',
                                   '<commit_before>', '<commit_msg>', '<commit_after>', '<jupyter_start>', '<jupyter_text>',
                                   '<jupyter_code>', '<jupyter_output>', '<empty_output>'], special_tokens=True)

        print("Creat model...")
        self.model = AutoModelForCausalLM.from_pretrained(mode_path, torch_dtype=torch.bfloat16, trust_remote_code=True).cuda()

    def _call(self, prompt: str, stop: Optional[List[str]] = None,
              run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any) -> str:
        prompt = prompt.strip()
        prompt += "<sep>"
        inputs = self.tokenizer(prompt, return_tensors="pt")["input_ids"].cuda()
        outputs = self.model.generate(inputs, do_sample=False, max_length=4096)
        output = self.tokenizer.decode(outputs[0])
        response = output.split("<sep>")[-1].split("<eod>")[0]
        return response

    @property
    def _llm_type(self) -> str:
        return "Yuan2_LLM"

class TextCorrector:
    def __init__(self, llm, embeddings):
        self.llm = llm
        self.embeddings = embeddings
        self.template = """

æˆ‘å¸Œæœ›ä½ èƒ½å……å½“åšå®¢æ–‡ç« æ¶¦è‰²åŠ©æ‰‹ã€‚æœ€ç»ˆä»…è¾“å‡ºæ¶¦è‰²åçš„æ–‡ç« ï¼Œè¯·æ£€æŸ¥ä»¥ä¸‹æ–‡æœ¬å¹¶å¯¹ç¬¬ä¸€æ­¥å¥å­å¢åŠ æ ‡ç‚¹ï¼Œç¬¬äºŒæ­¥å¯¹å¥å­ç»“æ„ã€è¯­æ³•ã€ç”¨è¯å’Œè¡¨è¾¾æ¸…æ™°åº¦ç­‰è¿›è¡Œæ”¹è¿›ï¼Œç›®çš„æ˜¯æé«˜æ–‡ç« çš„è´¨é‡å’Œå¯è¯»æ€§ï¼Œæ–‡æœ¬ï¼š{replace_text}`
è¡¥å……ï¼šè¯·ä¸è¦è¾“å‡ºå…¶ä»–çš„è¯­å¥ï¼Œä»…è¾“å‡ºæ–‡ç« ã€‚


"""
        self.prompt = PromptTemplate(
            input_variables=["replace_text"],
            template=self.template
        )
        self.chain = LLMChain(llm=self.llm, prompt=self.prompt)

    def polish_text(self, replace_text):
        polished_text = self.chain.run({
            "replace_text": replace_text
        })
        return polished_text

class TextGenerator:
    def __init__(self, llm):
        self.llm = llm
        self.prompt = PromptTemplate(
            input_variables=["After_text"],
            template="""
è¯·ä½ ä½œä¸ºMarkdownä¸“å®¶ï¼šè´Ÿè´£å¤„ç†å’Œç”ŸæˆMarkdownæ ¼å¼çš„å†…å®¹ï¼›
æ–‡æœ¬åˆ†æä¸“å®¶ï¼šè´Ÿè´£è¯†åˆ«å’Œæå–æ–‡æœ¬ä¸­çš„ç‰¹å®šä¿¡æ¯ï¼›
å†…å®¹ç¼–è¾‘ä¸“å®¶ï¼šè´Ÿè´£æ•´ç†å’Œç¼–è¾‘æ–‡æœ¬å†…å®¹ï¼Œä½¿å…¶ç¬¦åˆMarkdownæ ¼å¼ã€‚

    *æ ¹æ®`æˆ‘çš„æ–‡æœ¬`{After_text}çš„å†…å®¹ï¼Œè¿›è¡Œä»¥ä¸‹æ­¥éª¤ï¼š
    ç¬¬ä¸€æ­¥ï¼Œè¯†åˆ«å‡ºçš„ç‰¹å®šè¯­å¥æå–Markdownæ ‡é¢˜ã€æ—¥æœŸã€ç±»å‹ã€æ ‡ç­¾ï¼Œæ›¿æ¢Markdownå¤´éƒ¨çš„å¯¹åº”å­—æ®µ
    ç¬¬äºŒæ­¥ï¼Œå¯¹`Markdownå¤´éƒ¨`ï¼ˆ---ä¸---çš„éƒ¨åˆ†ï¼‰ï¼Œå¯¹`Markdownä¸»ä½“`ï¼ˆå¤´éƒ¨ä»¥å¤–çš„éƒ¨åˆ†ï¼‰è¿›è¡ŒReadmeæ–‡æ¡£æ ¼å¼ä¿®æ”¹ï¼Œæœ€ç»ˆä»…è¾“å‡ºå®Œæ•´Markdownæ ¼å¼æºä»£ç ã€‚
*è¡¥å……ï¼šè¯·ä¸è¦è¾“å‡ºå…¶ä»–çš„è¯­å¥ï¼Œä»…è¾“å‡ºå®Œæ•´Markdownæ ¼å¼æºä»£ç ã€‚
*ä»¥ä¸‹ä»…æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š
***
`æˆ‘çš„æ–‡æœ¬`ï¼š
å¤§å®¶å¥½,ä»Šå¤©çš„åšå®¢è®²çš„æ˜¯Dockerçš„èƒŒæ™¯çŸ¥è¯†ï¼Œåšå®¢æ ‡é¢˜ä¸ºç¬¬ä¸€ç¯‡Dockerçš„èƒŒæ™¯çŸ¥è¯†ï¼Œåšå®¢æ—¥æœŸ2024å¹´7æœˆ20æ—¥ï¼Œåšå®¢ç±»å‹ä¸ºDockerï¼Œåšå®¢æ ‡ç­¾ä¸ºèƒŒæ™¯çŸ¥è¯†ã€‚
ä¼ ç»Ÿçš„éƒ¨ç½²æ–¹å¼ï¼Œå¾€å¾€æ˜¯ç”¨ä¸€å †å¸®åŠ©æ–‡æ¡£ï¼Œå®‰è£…ç¨‹åºã€‚è€ŒDockerä½¿ç”¨æ‰“åŒ…é•œåƒå‘å¸ƒæµ‹è¯•ï¼Œèƒ½ä¸€é”®è¿è¡Œæ›´ä¾¿æ·çš„å‡çº§å’Œæ‰©ç¼©å®¹,ä½¿ç”¨äº†Dockerä¹‹åï¼Œæˆ‘ä»¬éƒ¨ç½²å¼•ç”¨å°±å’Œæ­ç§¯æœ¨ä¸€æ ·ï¼éå¸¸çš„ç®€å•ã€‚å½“é¡¹ç›®æ‰“åŒ…ä¸ºä¸€ä¸ªé•œåƒï¼Œå¯ä»¥æ‰©å±•åˆ°â€”â€”æœåŠ¡å™¨Aï¼æœåŠ¡å™¨Bï¼è¿™æ ·å®ç°äº†æ›´ç®€å•çš„ç³»ç»Ÿè¿ç»´,å®¹å™¨åŒ–ä¹‹åï¼Œæˆ‘ä»¬å¼€å‘ã€æµ‹è¯•ç¯å¢ƒéƒ½æ˜¯é«˜åº¦ä¸€è‡´çš„ï¼Œè¿™æ ·èƒ½å®ç°æ›´é«˜æ•ˆçš„è®¡ç®—èµ„æºåˆ©ç”¨ã€‚æœ¬è´¨ä¸ŠDockeræ˜¯å†…æ ¸çº§åˆ«çš„è™šæ‹ŸåŒ–ï¼Œå¯ä»¥åœ¨ä¸€ä¸ªç‰©ç†æœºä¸Šè¿è¡Œå¾ˆå¤šçš„å®¹å™¨å®ä¾‹ï¼æœåŠ¡å™¨çš„æ€§èƒ½èƒ½è¢«å‹æ¦¨åˆ°æè‡´ã€‚
`ä½ çš„è¾“å‡ºï¼ˆå®Œæ•´Markdownæ ¼å¼æºä»£ç ï¼‰`ï¼š
        ---
        title: ç¬¬ä¸€ç¯‡Dockerçš„èƒŒæ™¯çŸ¥è¯†
        date: 2024-07-20
        categories:
        - Docker
        tags:
        - èƒŒæ™¯çŸ¥è¯†
        ---

        # Dockerçš„èƒŒæ™¯çŸ¥è¯†
        # å¼•è¨€
        å¤§å®¶å¥½,ä»Šå¤©çš„åšå®¢è®²çš„æ˜¯Dockerçš„èƒŒæ™¯çŸ¥è¯†ï¼Œåšå®¢æ ‡é¢˜ä¸ºç¬¬ä¸€ç¯‡Dockerçš„èƒŒæ™¯çŸ¥è¯†ã€‚
        ***
        # Dockerçš„ä¼˜ç‚¹

        ##æ›´å¿«é€Ÿçš„äº¤ä»˜å’Œéƒ¨ç½²

        ###ä¼ ç»Ÿçš„éƒ¨ç½²æ–¹å¼
        å¾€å¾€æ˜¯ç”¨ä¸€å †å¸®åŠ©æ–‡æ¡£ï¼Œå®‰è£…ç¨‹åºã€‚
        ###Dockerçš„éƒ¨ç½²æ–¹å¼
        è€ŒDockerä½¿ç”¨æ‰“åŒ…é•œåƒå‘å¸ƒæµ‹è¯•ï¼Œèƒ½ä¸€é”®è¿è¡Œæ›´ä¾¿æ·çš„å‡çº§å’Œæ‰©ç¼©å®¹ã€‚

        ##æ›´ä¾¿æ·çš„å‡çº§å’Œæ‰©ç¼©å®¹
    Dockerä½¿ç”¨æ‰“åŒ…é•œåƒå‘å¸ƒæµ‹è¯•ï¼Œèƒ½ä¸€é”®è¿è¡Œæ›´ä¾¿æ·çš„å‡çº§å’Œæ‰©ç¼©å®¹,ä½¿ç”¨äº†Dockerä¹‹åï¼Œæˆ‘ä»¬éƒ¨ç½²å¼•ç”¨å°±å’Œæ­ç§¯æœ¨ä¸€æ ·ï¼éå¸¸çš„ç®€å•ã€‚
        ## æ›´ä¾¿æ·çš„å‡çº§å’Œæ‰©ç¼©å®¹
    å½“é¡¹ç›®æ‰“åŒ…ä¸ºä¸€ä¸ªé•œåƒï¼Œå¯ä»¥æ‰©å±•åˆ°â€”â€”æœåŠ¡å™¨Aï¼æœåŠ¡å™¨Bï¼è¿™æ ·å®ç°äº†æ›´ç®€å•çš„ç³»ç»Ÿè¿ç»´,
        ## æ›´ç®€å•çš„ç³»ç»Ÿè¿ç»´
    å®¹å™¨åŒ–ä¹‹åï¼Œæˆ‘ä»¬å¼€å‘ã€æµ‹è¯•ç¯å¢ƒéƒ½æ˜¯é«˜åº¦ä¸€è‡´çš„ï¼Œè¿™æ ·èƒ½å®ç°æ›´é«˜æ•ˆçš„è®¡ç®—èµ„æºåˆ©ç”¨ã€‚
        ##æ›´é«˜æ•ˆçš„è®¡ç®—èµ„æºåˆ©ç”¨
    Dockeræ˜¯å†…æ ¸çº§åˆ«çš„è™šæ‹ŸåŒ–ï¼Œå¯ä»¥åœ¨ä¸€ä¸ªç‰©ç†æœºä¸Šè¿è¡Œå¾ˆå¤šçš„å®¹å™¨å®ä¾‹ï¼æœåŠ¡å™¨çš„æ€§èƒ½èƒ½è¢«å‹æ¦¨åˆ°æè‡´ã€‚
***


            """

        )
        self.chain = LLMChain(llm=self.llm, prompt=self.prompt)

    def generate_text(self, text):
        generated_text = self.chain.run({
            "After_text": text
        })
        return generated_text

class KnowledgeBase:
    def __init__(self, embeddings):
        self.embeddings = embeddings
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=450,
            chunk_overlap=10,
            length_function=len
        )
        self.db = None

    def load_documents(self, file_path):
        if file_path.endswith('.pdf'):
            loader = PyPDFLoader(file_path)
        elif file_path.endswith('.txt'):
            loader = TextLoader(file_path)
        elif file_path.endswith('.docx'):
            loader = Docx2txtLoader(file_path)
        else:
            raise ValueError("ä¸Šä¼ æ ¼å¼è¯·ä»¥ PDF, TXT, æˆ– DOCX æ–‡ä»¶.")
        documents = loader.load_and_split(self.text_splitter)
        return documents

    def vectorize_documents(self, documents):
        self.db = FAISS.from_documents(documents, self.embeddings)

    def replace_similar_texts(self, speech_text, top_k=1):
        docs = self.db.similarity_search(speech_text, k=top_k)
        similar_text = " ".join([doc.page_content for doc in docs])
# æ›¿æ¢é€»è¾‘
        replaced_text = speech_text
        for doc in docs:
            if doc.page_content in speech_text:
                replaced_text = replaced_text.replace(doc.page_content, doc.page_content)

        return replaced_text, similar_text

@st.cache_resource
def get_models():
    llm = Yuan2_LLM(model_path)
    model_kwargs = {'device': 'cuda'}
    encode_kwargs = {'normalize_embeddings': True}
    embeddings = HuggingFaceEmbeddings(
        model_name=embedding_model_path,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs,
    )
    return llm, embeddings

def recognize_speech(audio_file):
    temp_audio_path = tempfile.mktemp(suffix=".wav")
    with open(temp_audio_path, "wb") as temp_audio:
        temp_audio.write(audio_file.read())
    model = whisper.load_model("large-v2")
    result = model.transcribe(temp_audio_path)
    return result['text']

def main():
    st.title("ğŸ—’ï¸BlogScribe")

    llm, embeddings = get_models()
    knowledge_base = KnowledgeBase(embeddings)
    text_corrector = TextCorrector(llm, embeddings)
    text_generator = TextGenerator(llm)

    if 'speech_text' not in st.session_state:
        st.session_state.speech_text = ""
    if 'similar_text' not in st.session_state:
        st.session_state.similar_text = ""
    if 'replaced_text' not in st.session_state:
        st.session_state.replaced_text = ""
    if 'polished_text' not in st.session_state:
        st.session_state.polished_text = ""
    if 'markdown_text' not in st.session_state:
        st.session_state.markdown_text = ""

    col1, col2 = st.columns([1, 2])

    with col1:
        st.header("ä¸Šä¼ åŒº")
        uploaded_file = st.file_uploader("ä¸Šä¼ çŸ¥è¯†åº“çš„æ–‡ä»¶ (PDF, TXT, DOCX)", type=['pdf', 'txt', 'docx'])
        audio_file = st.file_uploader("ä¸Šä¼ è¯­éŸ³è¾“å…¥çš„åšå®¢éŸ³é¢‘(wav, mp3, ogg)", type=['wav', 'mp3', 'ogg'])

        if uploaded_file:
            file_content = uploaded_file.read()
            temp_file_path = "temp." + uploaded_file.name.split('.')[-1]
            with open(temp_file_path, "wb") as temp_file:
                temp_file.write(file_content)
            docs = knowledge_base.load_documents(temp_file_path)
            knowledge_base.vectorize_documents(docs)
            st.success("æ–‡æ¡£åŠ è½½å¹¶å‘é‡åŒ–å®Œæˆ")

        st.header("ä¸‹è½½åŒº")
        if st.session_state.markdown_text:
            if st.button("ä¸‹è½½Markdownæ–‡ä»¶"):
                markdown_text = st.session_state.markdown_text
                markdown_file = tempfile.mktemp(suffix=".md")
                with open(markdown_file, "w") as file:
                    file.write(markdown_text)
                with open(markdown_file, "rb") as file:
                    st.download_button("ç‚¹å‡»ä¸‹è½½Markdownæ–‡ä»¶", data=file, file_name="blog_post.md", mime="text/markdown")

    with col2:
        st.header("å¤„ç†åŒº")

        st.text_area("è¯­éŸ³è½¬æ–‡æœ¬ç»“æœ", value=st.session_state.speech_text, height=150, key="speech_text_display")
        if st.button("è¯­éŸ³è¯†åˆ«"):
            if audio_file:
                st.session_state.speech_text = recognize_speech(audio_file)
                st.experimental_rerun()

        st.text_area("æ£€ç´¢åˆ°çš„ç›¸ä¼¼æ–‡æœ¬", value=st.session_state.similar_text, height=150, key="similar_text_display")
        st.text_area("æ›¿æ¢åçš„æ–‡æœ¬", value=st.session_state.replaced_text, height=150, key="replaced_text_display")
        if st.button("ç›¸ä¼¼æ–‡æœ¬æ£€ç´¢æ›¿æ¢"):
            if st.session_state.speech_text:
                replaced_text, similar_text = knowledge_base.replace_similar_texts(st.session_state.speech_text)
                st.session_state.similar_text = similar_text
                st.session_state.replaced_text = replaced_text
                st.experimental_rerun()
            else:
                st.error("è¯·å…ˆè¿›è¡Œè¯­éŸ³è¯†åˆ«")

        if st.button("æ¶¦è‰²æ–‡æœ¬"):
            if st.session_state.replaced_text:
                st.session_state.polished_text = text_corrector.polish_text(st.session_state.replaced_text)
                st.experimental_rerun()
            else:
                st.error("è¯·å…ˆè¿›è¡Œç›¸ä¼¼æ–‡æœ¬æ£€ç´¢æ›¿æ¢")

        st.text_area("æ¶¦è‰²åçš„æ–‡æœ¬", value=st.session_state.polished_text, height=150, key="polished_text_display")

        st.text_area("Markdownæ ¼å¼åšå®¢", value=st.session_state.markdown_text, height=150, key="markdown_text_display")
        if st.button("ç”ŸæˆMarkdownæ ¼å¼çš„åšå®¢"):
            if st.session_state.polished_text:
                st.session_state.markdown_text = text_generator.generate_text(st.session_state.polished_text)
                st.experimental_rerun()
            else:
                st.error("è¯·å…ˆæ¶¦è‰²æ–‡æœ¬")

if __name__ == '__main__':
    main()